{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Slope Prediction for Feed-Forward Process Control (without time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import sklearn.model_selection\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import sklearn, sklearn.preprocessing, sklearn.pipeline, sklearn.model_selection\n",
    "import sklearn.svm, sklearn.tree, sklearn.ensemble, sklearn.neural_network\n",
    "import sklearn.multioutput\n",
    "import numpy\n",
    "import functools, operator, itertools\n",
    "import time\n",
    "import scipy.stats, scipy\n",
    "# import scikitlearn_plus.neural_network\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Starting Data\n",
    "This data was generated from the data processing notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the smooth data: 856 rows by 24 columns\n"
     ]
    }
   ],
   "source": [
    "home_dir = os.getcwd().split('/notebooks')[0]\n",
    "sys.path.append(home_dir)\n",
    "home_dir\n",
    "smooth_data = pd.read_csv(f'{home_dir}/processed_data/smooth_data.csv')\n",
    "smooth_data.set_index(['composition','trial','time'], drop=True, inplace=True)\n",
    "print(f'Shape of the smooth data: {smooth_data.shape[0]} rows by {smooth_data.shape[1]} columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create train/test sets (Changed from original train/test\n",
    "New: <br>\n",
    "Training data comes from compostions 1-5, 9, 10.<br>\n",
    "Test data comes from compostions 6, 7, and 8.\n",
    "<br>\n",
    "<br>\n",
    "Old: <br>\n",
    "Training data comes from compostions 1-7.<br>\n",
    "Test data comes from compostions 8, 9, and 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the train_validation data: 550 rows by 24 columns\n",
      "Shape of the test data: 306 rows by 24 columns\n"
     ]
    }
   ],
   "source": [
    "train_validation_data = smooth_data.loc[[1,2,3,4,5,9,10]]\n",
    "print(f'Shape of the train_validation data: {train_validation_data.shape[0]} rows by {train_validation_data.shape[1]} columns')\n",
    "test_data = smooth_data.loc[[6,7,9]]\n",
    "print(f'Shape of the test data: {test_data.shape[0]} rows by {test_data.shape[1]} columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a function that generates the X array and y array for ML model training\n",
    "\n",
    "The parameter 'input_data' is used to specify whether raw data or the polynomial smoothed data will be used to train the model <br>\n",
    "The parameter 'conditions_to_include' is a list of the conditions to include in the returned arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_X_y_arrays(data):\n",
    "\n",
    "    data_copy = data.copy()\n",
    "    data_copy.reset_index(inplace=True)\n",
    "    X = data_copy [['acetate', 'biomass', 'butanol', 'butyrate', 'ethanol', 'CO', 'CO2', 'H2', 'flow rate']]\n",
    "    y = data_copy [['acetate_Δ', 'biomass_Δ', 'butanol_Δ', 'butyrate_Δ', 'ethanol_Δ']]\n",
    " \n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the train_validation X array: 550 rows by 9 columns\n",
      "Shape of the train_validation y array: 550 rows by 5 columns\n",
      "Shape of the test X array: 306 rows by 9 columns\n",
      "Shape of the test y array: 306 rows by 5 columns\n"
     ]
    }
   ],
   "source": [
    "X_train_validation, y_train_validation = get_X_y_arrays(train_validation_data)\n",
    "X_test, y_test = get_X_y_arrays(test_data)\n",
    "\n",
    "print(f'Shape of the train_validation X array: {X_train_validation.shape[0]} rows by {X_train_validation.shape[1]} columns')\n",
    "print(f'Shape of the train_validation y array: {y_train_validation.shape[0]} rows by {y_train_validation.shape[1]} columns')\n",
    "print(f'Shape of the test X array: {X_test.shape[0]} rows by {X_test.shape[1]} columns')\n",
    "print(f'Shape of the test y array: {y_test.shape[0]} rows by {y_test.shape[1]} columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train 25 different models using 5 algorithms for each of the 5 outputs\n",
    "algorithms = gradient boosting, random forest, support vector, neural net, lasso <br>\n",
    "outputs = acetate, biomass, butanol, butyrate, ethanol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acetate\n",
      "\n",
      "nn\n",
      "Fitting 10 folds for each of 3 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=30)]: Using backend LokyBackend with 30 concurrent workers.\n",
      "[Parallel(n_jobs=30)]: Done   4 out of  30 | elapsed:    8.8s remaining:   57.0s\n",
      "[Parallel(n_jobs=30)]: Done  15 out of  30 | elapsed:   10.6s remaining:   10.6s\n",
      "[Parallel(n_jobs=30)]: Done  26 out of  30 | elapsed:   12.3s remaining:    1.9s\n",
      "[Parallel(n_jobs=30)]: Done  30 out of  30 | elapsed:   12.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV score: 0.557:\n",
      "Best parameters: {'activation': 'relu', 'max_iter': 400} \n",
      "\n",
      "svm_rbf\n",
      "Fitting 10 folds for each of 4 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=30)]: Using backend LokyBackend with 30 concurrent workers.\n",
      "[Parallel(n_jobs=30)]: Done   9 out of  40 | elapsed:    0.1s remaining:    0.2s\n",
      "[Parallel(n_jobs=30)]: Done  40 out of  40 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=30)]: Using backend LokyBackend with 30 concurrent workers.\n",
      "[Parallel(n_jobs=30)]: Done   3 out of  10 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=30)]: Done   7 out of  10 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=30)]: Done  10 out of  10 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=30)]: Using backend LokyBackend with 30 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV score: 0.164:\n",
      "Best parameters: {'C': 1, 'epsilon': 0.1} \n",
      "\n",
      "rf\n",
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
      "Best CV score: 0.699:\n",
      "Best parameters: {'max_depth': 2, 'n_estimators': 10} \n",
      "\n",
      "biomass\n",
      "\n",
      "nn\n",
      "Fitting 10 folds for each of 3 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=30)]: Done   4 out of  30 | elapsed:    0.8s remaining:    5.4s\n",
      "[Parallel(n_jobs=30)]: Done  15 out of  30 | elapsed:    1.1s remaining:    1.1s\n",
      "[Parallel(n_jobs=30)]: Done  26 out of  30 | elapsed:    1.3s remaining:    0.2s\n",
      "[Parallel(n_jobs=30)]: Done  30 out of  30 | elapsed:    1.4s finished\n",
      "[Parallel(n_jobs=30)]: Using backend LokyBackend with 30 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV score: 0.237:\n",
      "Best parameters: {'activation': 'relu', 'max_iter': 400} \n",
      "\n",
      "svm_rbf\n",
      "Fitting 10 folds for each of 4 candidates, totalling 40 fits\n",
      "Best CV score: 0.127:\n",
      "Best parameters: {'C': 1, 'epsilon': 0.1} \n",
      "\n",
      "rf\n",
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
      "Best CV score: 0.401:\n",
      "Best parameters: {'max_depth': 2, 'n_estimators': 10} \n",
      "\n",
      "butanol\n",
      "\n",
      "nn\n",
      "Fitting 10 folds for each of 3 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=30)]: Done   9 out of  40 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=30)]: Done  23 out of  40 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=30)]: Done  37 out of  40 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=30)]: Done  40 out of  40 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=30)]: Using backend LokyBackend with 30 concurrent workers.\n",
      "[Parallel(n_jobs=30)]: Done   3 out of  10 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=30)]: Done   7 out of  10 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=30)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=30)]: Using backend LokyBackend with 30 concurrent workers.\n",
      "[Parallel(n_jobs=30)]: Done   4 out of  30 | elapsed:    7.5s remaining:   48.6s\n",
      "[Parallel(n_jobs=30)]: Done  15 out of  30 | elapsed:    9.7s remaining:    9.7s\n",
      "[Parallel(n_jobs=30)]: Done  26 out of  30 | elapsed:   10.4s remaining:    1.6s\n",
      "[Parallel(n_jobs=30)]: Done  30 out of  30 | elapsed:   10.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV score: 0.846:\n",
      "Best parameters: {'activation': 'relu', 'max_iter': 400} \n",
      "\n",
      "svm_rbf\n",
      "Fitting 10 folds for each of 4 candidates, totalling 40 fits\n",
      "Best CV score: 0.567:\n",
      "Best parameters: {'C': 1, 'epsilon': 1} \n",
      "\n",
      "rf\n",
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
      "Best CV score: 0.680:\n",
      "Best parameters: {'max_depth': 2, 'n_estimators': 10} \n",
      "\n",
      "butyrate\n",
      "\n",
      "nn\n",
      "Fitting 10 folds for each of 3 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=30)]: Using backend LokyBackend with 30 concurrent workers.\n",
      "[Parallel(n_jobs=30)]: Done   9 out of  40 | elapsed:    0.1s remaining:    0.2s\n",
      "[Parallel(n_jobs=30)]: Done  23 out of  40 | elapsed:    0.1s remaining:    0.1s\n",
      "[Parallel(n_jobs=30)]: Done  37 out of  40 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=30)]: Done  40 out of  40 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=30)]: Using backend LokyBackend with 30 concurrent workers.\n",
      "[Parallel(n_jobs=30)]: Done   3 out of  10 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=30)]: Done   7 out of  10 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=30)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=30)]: Using backend LokyBackend with 30 concurrent workers.\n",
      "[Parallel(n_jobs=30)]: Done   4 out of  30 | elapsed:    6.0s remaining:   39.2s\n",
      "[Parallel(n_jobs=30)]: Done  15 out of  30 | elapsed:   10.2s remaining:   10.2s\n",
      "[Parallel(n_jobs=30)]: Done  26 out of  30 | elapsed:   11.0s remaining:    1.7s\n",
      "[Parallel(n_jobs=30)]: Done  30 out of  30 | elapsed:   11.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV score: 0.856:\n",
      "Best parameters: {'activation': 'relu', 'max_iter': 400} \n",
      "\n",
      "svm_rbf\n",
      "Fitting 10 folds for each of 4 candidates, totalling 40 fits\n",
      "Best CV score: 0.243:\n",
      "Best parameters: {'C': 1, 'epsilon': 1} \n",
      "\n",
      "rf\n",
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
      "Best CV score: 0.435:\n",
      "Best parameters: {'max_depth': 2, 'n_estimators': 10} \n",
      "\n",
      "ethanol\n",
      "\n",
      "nn\n",
      "Fitting 10 folds for each of 3 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=30)]: Using backend LokyBackend with 30 concurrent workers.\n",
      "[Parallel(n_jobs=30)]: Done   9 out of  40 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=30)]: Done  23 out of  40 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=30)]: Done  37 out of  40 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=30)]: Done  40 out of  40 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=30)]: Using backend LokyBackend with 30 concurrent workers.\n",
      "[Parallel(n_jobs=30)]: Done   3 out of  10 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=30)]: Done   7 out of  10 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=30)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=30)]: Using backend LokyBackend with 30 concurrent workers.\n",
      "[Parallel(n_jobs=30)]: Done   4 out of  30 | elapsed:    7.6s remaining:   49.7s\n",
      "[Parallel(n_jobs=30)]: Done  15 out of  30 | elapsed:    9.6s remaining:    9.6s\n",
      "[Parallel(n_jobs=30)]: Done  26 out of  30 | elapsed:   10.7s remaining:    1.6s\n",
      "[Parallel(n_jobs=30)]: Done  30 out of  30 | elapsed:   10.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV score: 0.569:\n",
      "Best parameters: {'activation': 'relu', 'max_iter': 400} \n",
      "\n",
      "svm_rbf\n",
      "Fitting 10 folds for each of 4 candidates, totalling 40 fits\n",
      "Best CV score: 0.379:\n",
      "Best parameters: {'C': 1, 'epsilon': 1} \n",
      "\n",
      "rf\n",
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
      "Best CV score: 0.633:\n",
      "Best parameters: {'max_depth': 2, 'n_estimators': 10} \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=30)]: Using backend LokyBackend with 30 concurrent workers.\n",
      "[Parallel(n_jobs=30)]: Done   9 out of  40 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=30)]: Done  23 out of  40 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=30)]: Done  37 out of  40 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=30)]: Done  40 out of  40 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=30)]: Using backend LokyBackend with 30 concurrent workers.\n",
      "[Parallel(n_jobs=30)]: Done   3 out of  10 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=30)]: Done   7 out of  10 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=30)]: Done  10 out of  10 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "def gen_NN_fixed_n_layers(n_layers, n_neurons, neuron_step):\n",
    "    \"\"\"Generate NN hidden_layer_sizes of n_layers and up to n_neurons per layer \n",
    "    \"\"\"\n",
    "    # print (n_layers)\n",
    "    if n_layers == 1: \n",
    "        return [[i] for i in range(neuron_step, n_neurons+1, neuron_step)]\n",
    "    else:\n",
    "        pairs =  [  (i,  tail) for tail in gen_NN_fixed_n_layers(n_layers-1, n_neurons+1, neuron_step) for i in range(neuron_step, n_neurons+1, neuron_step) ]\n",
    "        return [[i]+ t for (i, t) in pairs]\n",
    "\n",
    "# print (gen_NN_fixed_n_layers(4, 10, 5))\n",
    "\n",
    "def gen_NN_uni(n_layers, n_neurons, layer_step, neuron_step):\n",
    "    \"\"\"Generate hidden layers of various number of layers and number of neurons \n",
    "    \"\"\" \n",
    "    various_NNs = [ gen_NN_fixed_n_layers(i , n_neurons, neuron_step) for i in range(2, n_layers+1, layer_step)]\n",
    "    return  functools.reduce(operator.add, various_NNs)\n",
    "\n",
    "def model_selection(X, y):\n",
    "    trained_models = {}\n",
    "    model_cfgs = {\n",
    "        \"nn\":{\n",
    "            'estimator': sklearn.neural_network.MLPRegressor(shuffle=True),\n",
    "#            Test grid\n",
    "            'param_grid':   {\n",
    "                # hidden_layer_sizes made the search space many order of magnitudes larger\n",
    "                'activation':         ['tanh', 'logistic', 'relu'], \n",
    "                'max_iter':           [400*i for i in range(1, 2)]\n",
    "            }\n",
    "#             Full grid\n",
    "#             'param_grid':   {\n",
    "#                 'hidden_layer_sizes': gen_NN_uni(5, 100, 1, 10),  \n",
    "#                 'activation':         ['tanh', 'logistic', 'relu'], \n",
    "#                 'max_iter':           [400*i for i in range(1, 10, 2)]\n",
    "#             }                \n",
    "        },\n",
    "        \"svm_rbf\":{\n",
    "                'estimator': sklearn.svm.SVR(kernel='rbf'),\n",
    "#               Test grid\n",
    "                'param_grid':   {\n",
    "                    'C':       [10**i for i in range(-1, 1)], \n",
    "                    'epsilon': [10**i for i in range(-1, 1)],\n",
    "                }\n",
    "#                  Full grid\n",
    "#                 'param_grid':   {\n",
    "#                     'C':       [10**i for i in range(-5, 5)], \n",
    "#                     'epsilon': [10**i for i in range(-5, 5)],\n",
    "# #                     'gamma':   [10**i for i in range(-5, 5)] # gamma gave me an error\n",
    "#                 }\n",
    "        },\n",
    "\n",
    "        \"rf\":{\n",
    "            'estimator': sklearn.ensemble.RandomForestRegressor(),\n",
    "#            Test grid\n",
    "            'param_grid':   {\n",
    "                'n_estimators': [10*i for i in range(1, 2)],\n",
    "                'max_depth':     [2*i for i in range(1, 1+1)],\n",
    "            }\n",
    "#             Full grid \n",
    "#             'param_grid':   {\n",
    "#                 'n_estimators': [10*i for i in range(1, 20)],\n",
    "#                 'max_depth':     [2*i for i in range(20)], \n",
    "# #                'max_samples': [0.05*i for i in range(1, 10+1)] # max samples gave me an error\n",
    "#             }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # scale data once \n",
    "    Scaler = sklearn.preprocessing.MinMaxScaler()\n",
    "    X = Scaler.fit_transform(X, y)\n",
    "        \n",
    "    trained_model_dictionary = {}\n",
    "        \n",
    "    for index, output in enumerate(['acetate', 'biomass', 'butanol', 'butyrate', 'ethanol']):\n",
    "        print(f'{output}\\n')\n",
    "        for model_name, model_conf in model_cfgs.items():\n",
    "            print (model_name)\n",
    "            search = sklearn.model_selection.GridSearchCV(\n",
    "                estimator = model_conf[\"estimator\"], \n",
    "                param_grid = model_conf[\"param_grid\"], \n",
    "#                 scoring = [\"neg_mean_absolute_percentage_error\",\"r2\"],\n",
    "#                 neg_mean_absolute_percentage_error is not a valid, but neg_mean_absolute_error is\n",
    "                scoring = \"r2\",\n",
    "                refit = True,\n",
    "                cv = sklearn.model_selection.ShuffleSplit(n_splits=10, test_size=0.1, random_state=0), \n",
    "                n_jobs=30,\n",
    "                verbose=3\n",
    "            )\n",
    "        \n",
    "            # This line uses only the column related to the output rather than the whole output matrix\n",
    "            # Need to assign the slice to a new variable, so that data for other outputs is not lost\n",
    "            y_output=y[:,index]\n",
    "\n",
    "            search.fit(X, y_output)\n",
    "            print(\"Best CV score: %0.3f:\" % search.best_score_)\n",
    "            print(\"Best parameters:\",  search.best_params_, '\\n')\n",
    "            trained_models[model_name] = search \n",
    "\n",
    "        trained_model_dictionary[output] = trained_models\n",
    "        \n",
    "    return trained_model_dictionary\n",
    "\n",
    "trained_model_dictionary = model_selection(X_train_validation, y_train_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_models = {\n",
    "    \"acetate\": {\n",
    "        \"nn\": sklearn.neural_network.MLPRegressor(\n",
    "            shuffle=True, \n",
    "            activation = 'tanh', \n",
    "            hidden_layer_sizes = [20,100],\n",
    "            max_iter = 5000\n",
    "        ),\n",
    "        \"svm_rbf\": sklearn.svm.SVR(\n",
    "            kernel = 'rbf', \n",
    "            C = 10000, \n",
    "            epsilon = 0.1, \n",
    "            gamma = 10\n",
    "        ),\n",
    "        'rf': sklearn.ensemble.RandomForestRegressor(\n",
    "            max_depth = 30,\n",
    "#             max_samples = 0.5,\n",
    "            n_estimators = 60\n",
    "        )\n",
    "    },\n",
    "    \"biomass\": {\n",
    "        \"nn\": sklearn.neural_network.MLPRegressor(\n",
    "            shuffle=True, \n",
    "            activation = 'relu', \n",
    "            hidden_layer_sizes = [100, 100, 80],\n",
    "            max_iter = 5000\n",
    "        ),\n",
    "        \"svm_rbf\": sklearn.svm.SVR(\n",
    "            kernel = 'rbf', \n",
    "            C = 10, \n",
    "            epsilon = 0.0001, \n",
    "            gamma = 10\n",
    "        ),\n",
    "        'rf': sklearn.ensemble.RandomForestRegressor(\n",
    "            max_depth = 20,\n",
    "#             max_samples = 0.5,\n",
    "            n_estimators = 60\n",
    "        )\n",
    "    },\n",
    "    \"butanol\": {\n",
    "        \"nn\": sklearn.neural_network.MLPRegressor(\n",
    "            shuffle=True, \n",
    "            activation = 'tanh', \n",
    "            hidden_layer_sizes = [20, 100, 60, 20],\n",
    "            max_iter = 5000\n",
    "        ),\n",
    "        \"svm_rbf\": sklearn.svm.SVR(\n",
    "            kernel = 'rbf', \n",
    "            C = 10000, \n",
    "            epsilon = 0.01, \n",
    "            gamma = 10\n",
    "        ),\n",
    "        'rf': sklearn.ensemble.RandomForestRegressor(\n",
    "            max_depth = 12,\n",
    "#             max_samples = 0.5,\n",
    "            n_estimators = 40\n",
    "        )\n",
    "    },\n",
    "    \"butyrate\": {\n",
    "        \"nn\": sklearn.neural_network.MLPRegressor(\n",
    "            shuffle=True, \n",
    "            activation = 'tanh', \n",
    "            hidden_layer_sizes = [60, 60, 100, 20],\n",
    "            max_iter = 5000\n",
    "        ),\n",
    "        \"svm_rbf\": sklearn.svm.SVR(\n",
    "            kernel = 'rbf', \n",
    "            C = 1000, \n",
    "            epsilon = 0.01, \n",
    "            gamma = 10\n",
    "        ),\n",
    "        'rf': sklearn.ensemble.RandomForestRegressor(\n",
    "            max_depth = 34,\n",
    "#             max_samples = 0.5,\n",
    "            n_estimators = 80\n",
    "        )\n",
    "    },\n",
    "    \"ethanol\": {\n",
    "        \"nn\": sklearn.neural_network.MLPRegressor(\n",
    "            shuffle=True, \n",
    "            activation = 'tanh', \n",
    "            hidden_layer_sizes = [60, 100],\n",
    "            max_iter = 5000\n",
    "        ),\n",
    "        \"svm_rbf\": sklearn.svm.SVR(\n",
    "            kernel = 'rbf', \n",
    "            C = 10000, \n",
    "            epsilon = 0.1, \n",
    "            gamma = 10\n",
    "        ),\n",
    "        'rf': sklearn.ensemble.RandomForestRegressor(\n",
    "            max_depth = 38,\n",
    "#             max_samples = 0.5,\n",
    "            n_estimators = 170\n",
    "        )\n",
    "    },\n",
    "}\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPRegressor(activation='tanh', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "             beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "             hidden_layer_sizes=[20, 100], learning_rate='constant',\n",
      "             learning_rate_init=0.001, max_iter=5000, momentum=0.9,\n",
      "             n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
      "             random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
      "             validation_fraction=0.1, verbose=False, warm_start=False)\n",
      "\n",
      "SVR(C=10000, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma=10,\n",
      "    kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)\n",
      "\n",
      "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=30,\n",
      "                      max_features='auto', max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, n_estimators=60,\n",
      "                      n_jobs=None, oob_score=False, random_state=None,\n",
      "                      verbose=0, warm_start=False)\n",
      "\n",
      "\n",
      "MLPRegressor(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "             beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "             hidden_layer_sizes=[100, 100, 80], learning_rate='constant',\n",
      "             learning_rate_init=0.001, max_iter=5000, momentum=0.9,\n",
      "             n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
      "             random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
      "             validation_fraction=0.1, verbose=False, warm_start=False)\n",
      "\n",
      "SVR(C=10, cache_size=200, coef0=0.0, degree=3, epsilon=0.0001, gamma=10,\n",
      "    kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)\n",
      "\n",
      "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=20,\n",
      "                      max_features='auto', max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, n_estimators=60,\n",
      "                      n_jobs=None, oob_score=False, random_state=None,\n",
      "                      verbose=0, warm_start=False)\n",
      "\n",
      "\n",
      "MLPRegressor(activation='tanh', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "             beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "             hidden_layer_sizes=[20, 100, 60, 20], learning_rate='constant',\n",
      "             learning_rate_init=0.001, max_iter=5000, momentum=0.9,\n",
      "             n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
      "             random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
      "             validation_fraction=0.1, verbose=False, warm_start=False)\n",
      "\n",
      "SVR(C=10000, cache_size=200, coef0=0.0, degree=3, epsilon=0.01, gamma=10,\n",
      "    kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)\n",
      "\n",
      "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=12,\n",
      "                      max_features='auto', max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, n_estimators=40,\n",
      "                      n_jobs=None, oob_score=False, random_state=None,\n",
      "                      verbose=0, warm_start=False)\n",
      "\n",
      "\n",
      "MLPRegressor(activation='tanh', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "             beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "             hidden_layer_sizes=[60, 60, 100, 20], learning_rate='constant',\n",
      "             learning_rate_init=0.001, max_iter=5000, momentum=0.9,\n",
      "             n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
      "             random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
      "             validation_fraction=0.1, verbose=False, warm_start=False)\n",
      "\n",
      "SVR(C=1000, cache_size=200, coef0=0.0, degree=3, epsilon=0.01, gamma=10,\n",
      "    kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)\n",
      "\n",
      "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=34,\n",
      "                      max_features='auto', max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, n_estimators=80,\n",
      "                      n_jobs=None, oob_score=False, random_state=None,\n",
      "                      verbose=0, warm_start=False)\n",
      "\n",
      "\n",
      "MLPRegressor(activation='tanh', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "             beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "             hidden_layer_sizes=[60, 100], learning_rate='constant',\n",
      "             learning_rate_init=0.001, max_iter=5000, momentum=0.9,\n",
      "             n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
      "             random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
      "             validation_fraction=0.1, verbose=False, warm_start=False)\n",
      "\n",
      "SVR(C=10000, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma=10,\n",
      "    kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)\n",
      "\n",
      "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=38,\n",
      "                      max_features='auto', max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, n_estimators=170,\n",
      "                      n_jobs=None, oob_score=False, random_state=None,\n",
      "                      verbose=0, warm_start=False)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for output in ['acetate', 'biomass', 'butanol', 'butyrate', 'ethanol']:\n",
    "    for model in ['nn', 'svm_rbf', 'rf']:\n",
    "        \n",
    "        print(optimized_models[output][model])\n",
    "        print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Scores = [] # to store best scores from each type of models, the metric is the same as those used in training\n",
    "Predictions = [] # to store predictions from each type of models. \n",
    "\n",
    "for model_name, model in Trained_models.items():\n",
    "    print (\"using model \", model_name)\n",
    "    prediction = model.predict(X_test) # 2-D numpy array, each row is a sample, each column is one output\n",
    "    output_name = ['acetate_Δ', 'biomass_Δ', 'butanol_Δ', 'butyrate_Δ', 'ethanol_Δ']\n",
    "    for i in range(5):\n",
    "        (r, p) = scipy.stats.pearsonr(prediction[:,i], y_test[:,i])\n",
    "        print (output_name[i], \", pearson's CC :\", r)\n",
    "        # (r, p) = scipy.stats.spearmanr(prediction[:,i], y_test[:,i])\n",
    "        # print (output_name[i], \", spearman's' CC :\", r)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Below are from Garrett's original result \n",
    "\n",
    "## Plot evaluation metrics\n",
    "Plot r$^2$ values first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for output in ['acetate', 'biomass', 'butanol', 'butyrate', 'ethanol']:\n",
    "    plot_evaluation_metrics(output, 'r2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot normalized mean squared error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for output in ['acetate', 'biomass', 'butanol', 'butyrate', 'ethanol']:\n",
    "#     plot_evaluation_metrics(output, 'norm_rmse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot scatterplots of slope fits\n",
    "Plot alcohols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = ['ethanol', 'butanol']\n",
    "models = ['gradient boosting', 'random forest', 'support vector', 'neural net', 'lasso']\n",
    "\n",
    "fig, ax = plt.subplots(2, 5, figsize=(20, 8))\n",
    "fig.suptitle(f'Slope Predictions (without time)')\n",
    "\n",
    "for x in range(2):\n",
    "    for y in range(5):\n",
    "        ax[x, y].title.set_text(f'{outputs[x]} {models[y]}')\n",
    "        \n",
    "        ax[x, y].scatter(train_data[outputs[x]], train_predictions[models[y]][outputs[x]])\n",
    "        ax[x, y].scatter(validation_data[outputs[x]], validation_predictions[models[y]][outputs[x]])\n",
    "        ax[x, y].scatter(test_data[outputs[x]], test_predictions[models[y]][outputs[x]])\n",
    "        \n",
    "        minimum = min(pd.concat([\n",
    "            test_data[outputs[x]],\n",
    "            validation_data[outputs[x]],\n",
    "            train_data[outputs[x]], \n",
    "            test_predictions[models[y]][outputs[x]],\n",
    "            validation_predictions[models[y]][outputs[x]],\n",
    "            train_predictions[models[y]][outputs[x]]\n",
    "        ], axis=0))\n",
    "\n",
    "        maximum = max(pd.concat([\n",
    "            test_data[outputs[x]],\n",
    "            validation_data[outputs[x]],\n",
    "            train_data[outputs[x]],\n",
    "            test_predictions[models[y]][outputs[x]],\n",
    "            validation_predictions[models[y]][outputs[x]],\n",
    "            train_predictions[models[y]][outputs[x]]\n",
    "        ], axis=0))\n",
    "\n",
    "        ax[x, y].plot([minimum, maximum], [minimum, maximum], 'r') #row=0, col=0\n",
    "\n",
    "plt.savefig(f'{home_dir}/figures/slope_figures_with_time/slope_scatterplots_without_time.png', dpi=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = ['acetate', 'butyrate', 'biomass']\n",
    "models = ['gradient boosting', 'random forest', 'support vector', 'neural net', 'lasso']\n",
    "\n",
    "fig, ax = plt.subplots(3, 5, figsize=(20, 12))\n",
    "fig.suptitle(f'Slope Predictions (without time)')\n",
    "\n",
    "for x in range(3):\n",
    "    for y in range(5):\n",
    "        ax[x, y].title.set_text(f'{outputs[x]} {models[y]}')\n",
    "        \n",
    "        ax[x, y].scatter(train_data[outputs[x]], train_predictions[models[y]][outputs[x]])\n",
    "        ax[x, y].scatter(validation_data[outputs[x]], validation_predictions[models[y]][outputs[x]])\n",
    "        ax[x, y].scatter(test_data[outputs[x]], test_predictions[models[y]][outputs[x]])\n",
    "        \n",
    "        minimum = min(pd.concat([\n",
    "            test_data[outputs[x]],\n",
    "            validation_data[outputs[x]],\n",
    "            train_data[outputs[x]], \n",
    "            test_predictions[models[y]][outputs[x]],\n",
    "            validation_predictions[models[y]][outputs[x]],\n",
    "            train_predictions[models[y]][outputs[x]]\n",
    "        ], axis=0))\n",
    "\n",
    "        maximum = max(pd.concat([\n",
    "            test_data[outputs[x]],\n",
    "            validation_data[outputs[x]],\n",
    "            train_data[outputs[x]],\n",
    "            test_predictions[models[y]][outputs[x]],\n",
    "            validation_predictions[models[y]][outputs[x]],\n",
    "            train_predictions[models[y]][outputs[x]]\n",
    "        ], axis=0))\n",
    "\n",
    "        ax[x, y].plot([minimum, maximum], [minimum, maximum], 'r') #row=0, col=0\n",
    "\n",
    "plt.savefig(f'{home_dir}/figures/slope_figures_without_time/slope_scatterplots_without_time_all.png', dpi=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance\n",
    "Define a function to get feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_importances(model):\n",
    "    outputs = ['acetate', 'biomass', 'butanol', 'butyrate', 'ethanol']\n",
    "    features = ['acetate', 'biomass', 'butanol', 'butyrate', 'ethanol', 'CO', 'CO2', 'H2', 'flow rate']\n",
    "\n",
    "    array_list = []\n",
    "\n",
    "    for i in range(5):\n",
    "        feature_importance_array = model.estimators_[i].steps[1][1].best_estimator_.feature_importances_\n",
    "        array_list.append(list(feature_importance_array))\n",
    "    df = pd.DataFrame(array_list, columns = features, index = outputs)  \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get feature importance values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = [\n",
    "    trained_models['gradient boosting'],\n",
    "    trained_models['random forest'],\n",
    "    ]\n",
    "\n",
    "for model in model_list:\n",
    "    display(get_feature_importances(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to plot feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importance_metabolites(model_name):\n",
    "    model = trained_models[model_name]\n",
    "    data = get_feature_importances(model)\n",
    "    \n",
    "    ethanol_data = data.iloc[4]\n",
    "    butanol_data = data.iloc[2]\n",
    "    \n",
    "    ethanol_data = ethanol_data[:5]\n",
    "    butanol_data = butanol_data[:5]\n",
    "    \n",
    "    labels = ['acetate', 'biomass', 'butanol', 'butyrate', 'ethanol'] #, 'CO', 'CO2', 'H2', 'flow rate']\n",
    "    \n",
    "    x = np.arange(len(labels))  # the label locations\n",
    "    width = 0.25  # the width of the bars\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8,5))\n",
    "    rects5 = ax.bar(x - 0.5*width, ethanol_data , width, label='Ethanol Rate')\n",
    "    rects3 = ax.bar(x + 0.5*width , butanol_data , width, label='Butanol Rate')\n",
    "    \n",
    "    # Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "    ax.set_ylabel(f'Importance')\n",
    "    ax.set_yscale('log')\n",
    "    ax.set_title(f'Feature Importance for {model_name} Slope Predictions (without time)')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(labels)\n",
    "    ax.legend()\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.savefig(f'{home_dir}/figures/slope_figures_without_time/slope_feature_importance_{model_name}_metabolites_without_time.png', dpi=100)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_importance_metabolites('gradient boosting')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot feature importance gradient boosting and random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_importance_metabolites('random forest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importance_gases(model_name):\n",
    "    model = trained_models[model_name]\n",
    "    data = get_feature_importances(model)\n",
    "    \n",
    "    ethanol_data = data.iloc[4]\n",
    "    butanol_data = data.iloc[2]\n",
    "    \n",
    "    ethanol_data = ethanol_data[5:]\n",
    "    butanol_data = butanol_data[5:]\n",
    "    \n",
    "    labels = ['CO', 'CO2', 'H2', 'flow rate']\n",
    "    \n",
    "    x = np.arange(len(labels))  # the label locations\n",
    "    width = 0.25  # the width of the bars\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8,5))\n",
    "    rects5 = ax.bar(x - 0.5*width, ethanol_data , width, label='Ethanol Rate')\n",
    "    rects3 = ax.bar(x + 0.5*width , butanol_data , width, label='Butanol Rate')\n",
    "\n",
    "    # Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "    ax.set_ylabel(f'Importance')\n",
    "    ax.set_yscale('log')\n",
    "    ax.set_title(f'Feature Importance for {model_name} Slope Predictions (without time)')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(labels)\n",
    "    ax.legend()\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    plt.savefig(f'{home_dir}/figures/slope_figures_without_time/slope_feature_importance_gases_{model_name}.png', dpi=100)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_importance_gases('gradient boosting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_importance_gases('random forest')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "biodesign_3.7",
   "language": "python",
   "name": "biod_3.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
